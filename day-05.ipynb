{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40e6e4ac-16b0-4236-959e-4dfc0140f5d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Read CSV\n",
    "oct_df = spark.read.csv(\n",
    "    \"/Volumes/workspace/ecommerce/ecommerce_data/2019-Oct.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "# Step 2: Set Delta table path\n",
    "delta_path = \"/Volumes/workspace/ecommerce/delta/ecommerce_events\"\n",
    "\n",
    "# Step 3: Save as Delta table (overwrite if exists)\n",
    "oct_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(delta_path)\n",
    "\n",
    "# Step 4: Import DeltaTable and Spark functions\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import col, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Step 5: Load Delta table\n",
    "delta_table = DeltaTable.forPath(spark, delta_path)\n",
    "\n",
    "# Step 6: Deduplicate source to avoid multiple source row errors\n",
    "window_spec = Window.partitionBy(\n",
    "    \"user_session\",\n",
    "    \"event_time\",\n",
    "    \"product_id\",\n",
    "    \"event_type\"\n",
    ").orderBy(col(\"event_time\").desc())\n",
    "\n",
    "updates_df = (\n",
    "    oct_df\n",
    "    .withColumn(\"rn\", row_number().over(window_spec))\n",
    "    .filter(col(\"rn\") == 1)\n",
    "    .drop(\"rn\")\n",
    ")\n",
    "\n",
    "# Step 7: Check count before merge\n",
    "before_count = spark.read.format(\"delta\").load(delta_path).count()\n",
    "print(\"Before merge count of rows:\", before_count)\n",
    "\n",
    "# Step 8: Perform MERGE\n",
    "delta_table.alias(\"t\").merge(\n",
    "    updates_df.alias(\"s\"),\n",
    "    \"\"\"\n",
    "    t.user_session = s.user_session AND\n",
    "    t.event_time = s.event_time AND\n",
    "    t.product_id = s.product_id AND\n",
    "    t.event_type = s.event_type\n",
    "    \"\"\"\n",
    ").whenMatchedUpdateAll() \\\n",
    " .whenNotMatchedInsertAll() \\\n",
    " .execute()\n",
    "\n",
    "# Step 9: Check count after merge\n",
    "after_count = spark.read.format(\"delta\").load(delta_path).count()\n",
    "print(\"After merge count of rows:\", after_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7196535-c3ff-4280-b1a1-11a5a6807f38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "QUERY VERSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f131eb3-eda7-4744-9edc-e73f39843383",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Load table\n",
    "delta_table = DeltaTable.forPath(spark, delta_path)\n",
    "\n",
    "# a) Show Delta history\n",
    "spark.sql(f\"DESCRIBE HISTORY delta.`{delta_path}`\").show(truncate=False)\n",
    "\n",
    "# b) Query an old version by version number\n",
    "version_0_df = spark.read.format(\"delta\") \\\n",
    "    .option(\"versionAsOf\", 0) \\\n",
    "    .load(delta_path)\n",
    "\n",
    "print(\"Number of rows in version 0:\", version_0_df.count())\n",
    "\n",
    "# c) Query as of a timestamp\n",
    "from datetime import datetime\n",
    "\n",
    "# Example: all rows as of Jan 1, 2026\n",
    "yesterday_df = spark.read.format(\"delta\") \\\n",
    "    .option(\"timestampAsOf\", \"2026-01-12 15:20:23\") \\\n",
    "    .load(delta_path)\n",
    "\n",
    "print(\"Number of rows as of timestamp:\", yesterday_df.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "174c22c9-653a-4f17-baa5-50ae1c27e1c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "OPTIMIZE TABLES (ZORDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f68fdce1-434e-485b-8af0-000a0491f177",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optimize Delta table with ZORDER by columns often queried\n",
    "spark.sql(f\"OPTIMIZE delta.`{delta_path}` ZORDER BY (event_type, user_id)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59548588-f996-4200-9b47-052eae9662ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "CLEAN OLD FILES (VACCUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eac0bcdb-7641-498c-9e82-b208bb8b3d33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# retention: 7 days (168 hours)\n",
    "delta_table.vacuum(retentionHours=168)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91d30d8a-a9ca-4393-b782-4174eb813d1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optimize Delta table with ZORDER by columns often queried\n",
    "spark.sql(f\"OPTIMIZE delta.`{delta_path}` ZORDER BY (event_type, user_id)\")\n",
    "\n",
    "# retention: 7 days (168 hours)\n",
    "delta_table.vacuum(retentionHours=168)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "day-05",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
